----[14-02-2018]------------------------------------------------------------

setup of client VMs complete

set new IPs for all VMs

corected bugs in disk_stress script

scripts to be made [start_stop_vm, run_command_vms, copy_config_script]

client nfs performance is too low

couldn't run phoronix on nfs mount

thinking of searching for an alternate to phoronix

----[15-02-2018]------------------------------------------------------------

found client network adapter performance to be optimal

hypervisor and clients are performing okay

started writing script to automate start/shutdown of VMs

VM manage script config template done:

{
    "host1_ip": {
        "vm_names": ["name_1", "name_2", ...]
    }

    "host2_ip": {
        "vm_names": ["name_3", "name_4", ...]
    }
}

modifying cachedir and environmentdir in user-config.xml allow running
phoronix on any mount point

----[16-02-2018]------------------------------------------------------------

started NFS benchmark with single client and no load, using pts/iozone

benchmark-config:
fstab [rw, sync, hard, intr 0 0]
exports [rw, root_squash, subtree_check]
record [all]
file [512MB, 2GB]

testing record [4KB, 64KB, 1MB] and file [512MB, 2GB], test [rw] is taking 
too long to complete with one test > 6 hours

phoronix test suite runs each test three or more times and takes mean to
find the results

found about btest, stress and filebench tools to simulate disk IO

not using NFSometer as it is a mere wrapper around existing disk tools

----[17-02-2018]------------------------------------------------------------

Started overnight benchmark using pts/iozone:
fstab [rw, sync, hard intr 0 0]
exports [rw, root_squash, subtree_check]
record [64KB]
file [512 MB]

----[18-02-2018]------------------------------------------------------------

First NFS test completed

NFS read performance at ~7700 MB/s, clearly incorrect

installed bonnie++ to test NFS performance

started bonnie++ benchmark:
fstab [rw, sync, hard intr 0 0]
exports [rw, root_squash, subtree_check]

bonnie++ benchmark writing one byte at a time

changed chunk size to 64K, still no change in "writing one byte at a time"

bonnie++ benchmark taking too long to complete, cancelling

----[19-02-2018]------------------------------------------------------------

Created new VM for client-1

installed dataplicity for remote administration

----[20-02-2018]------------------------------------------------------------

instlled phoronix on new VM

installed nfs-common

mounted nfs share as home directory

started using pts/dbench as benchmark utility

dbench is specifically meant for SMB/NFS benchmarking
--------------------------------------------------------------------------------
nfs_test_1:
client [1]

fstab [rw, sync, hard, intr 0 0]
exports [rw, no_root_squash, subtree_check]

removed root_squash as the nfs mount is now the home directory

dbench can emulate large number of NFS clients

dbench test complete quickly with ETA 40 minutes

nfs_test_1 completed with 0.29 MB/s NFS speed
---------------------------------------------------------------------------------------
nfs_test_2:
client [6]
fstab [rw, sync, hard, intr 0 0]
exports [rw, no_root_squash, subtree_check]

results in performance of 0.94 MB/s
----------------------------------------------------------------------------------------
nfs_test_3:
client [6]
fstab [rw, sync, hard, intr, nfsvers=4 0 0]
exports [rw, no_root_squash, subtree_check]

enforced NFS version 4

found that enforcing version 4 is not recommended

server by default uses highest NFS version available

lost contact to client-1 while modifying fstab

the error was due to FS not being unmounted cleanly
-----------------------------------------------------------------------------------------
nfs_test_3:
client [6]
fstab [rw, sync, hard, intr, rsize=16384, wsize=16384 0 0]
exports [rw, no_root_squash, subtree_check]

changed read, write block size = 16K

resulted in 0.94 MB/s -> no performance increment
----------------------------------------------------------------------------------------------
nfs_test_4:
client [6]
fstab [rw, sync, hard, intr, rsize=65536, wsize=65536 0 0]
exports [rw, no_root_squash, subtree_check]

changed read, write block size = 64K

resulted in 0.95 MB/s -> no performance increment

removed github/project_nfs moving it to gitlab

----[21-02-2018]------------------------------------------------------------

nfs_test_5:
client [6]
fstab [rw, sync, hard, intr, rsize=102400, wsize=102400 0 0]
exports [rw, no_root_squash, subtree_check]

resulted in 0.94 MB/s -> no performance increment

moved project_nfs to gitlab

started writing the log

client-1 MTU is set to 1500
--------------------------------------------------------------------------------------------------------
nfs_test_6:
client [6]
fstab [rw, sync, hard, intr, rsize=1024, wsize=1024 0 0]
exports [rw, no_root_squash, subtree_check]

reduced rsize, wsize to 1K

looking if rsize/wsize < MTU can increase performance

resulted in 0.92 MB/s -> marginal decrease in speed

packet fragmentation can occur if rsize/wsize > MTU. But in this case, it
is providing a marginal increment in performance

nfssat shows no retranmsissions
Client rpc stats:
calls      retrans    authrefrsh
2310766    0          2310766

/proc/sys/net/core/rmem_default and max values are sufficiently large

software development is attribute intensive env -> lot of small files

nfs server memory usage is very low, with used cache being half of memory
---------------------------------------------------------------------------------------------------
nfs_test_7:
client [6]
fstab [rw, sync, hard, intr, udp 0 0]
exports [rw, no_root_squash, subtree_check]

force NFS to run on UDP

many users report performance degradation after switching to NFSv4

phoronix-test-suite showing "pts/dbench already running" error

thinking of reinstalling client or cloning ldap_server

----[22-02-2018]------------------------------------------------------------

phoronix fails to run when using UDP

ran dbench manually, resulted in 0.77 MB/s -> decrease in performance

ran dbench manually on VM without UDP, resulted in 0.94 MB/s

there is no packet fragmentation, thus number of tweaks such as rsize, wsize
won't work

----[23-02-2018]------------------------------------------------------------

instructions on the web provide marginal/no performance improvement

disabling subtree check too see if there is performance improvement

nfs_test_8:
client [6]
fstab [rw, sync, hard, intr 0 0]
exports [rw, no_root_squash, no_subtree_check]

resulted in 0.94 MB/s -> no performance difference from earlier setup

----[24-02-2018]------------------------------------------------------------

many of the performance guideliness in websites are dated / do not work

nfs_test_9:
client [6]
fstab [rw, sync, hard, intr 0 0]
exports [rw, no_root_squash, no_subtree_check, async]

found that many of the websites have duplicated content

test resulted in 28.72 MB/s -> huge increase in performance

cache effect is obvious

running test with dd:
dd if=/dev/zero of=/home/user_1/tmp_file bsize=512M count=4 conv=fdatasync

resulted in 58.1 MB/s -> way over network card capabilities

running test with dd:
dd if=/dev/zero of=/home/user_1/tmp_file bsize=1M count=2048 conv=fdatasync

resulted in 32.9 MB/s -> still not accurate

running test with dd:
dd if=/dev/zero of=/home/user_1/tmp_file bsize=4K count=512000 conv=fdatasync

resulted in 14.1 MB/s -> the maximum speed should be ~ 12.5 MB/s

running test with dd:
dd if=/dev/zero of=/home/user_1/tmp_file bsize=1K count=2048000 conv=fdatasync

resulted in 7.1 MB/s -> thats more like it :D

server side async provides biggest performance boost

nfs_test_10:
client [6]
fstab [rw, sync, hard, intr 0 0]
exports [rw, no_root_squash, subtree_check, async]

testing if subtree check has performance impacts

resulted in 28.90 MB/s -> no major difference in performance

----[25-02-2018]------------------------------------------------------------

Suggesting improvement for client side async:
sync before client side memory is full and when light traffic

Suggesting improvement for server side async:
Have client side permanent buffer as journal, recover if server fails

first phase of research found that highest performing gain is from async

fault with server and client async is data loss

----[26-02-2018]------------------------------------------------------------

server side async accepts a file write command and immediatly provides
file written complete signal, while file is being written in background

this improves the write speed. It also improves the read speed as file can
now be read by the client without waiting for the server to update the
access timestamp on the read files

client side async uses main memory as a large buffer. files to be written
are stored in buffer till buffer is full or shutdown

server side async can cause data loss if server improperly shutdowns

server side async issue can be solved by implementing a client side temp 
journal on secondary media

Client --> Journal --> Server

Everything to be written to the server goes through the client HDD. In case
of server being not shutdown properly, the corrupt files can be recovered
from journal.

If corrupt shutdown happens, client and server prepares a list of files
based on file access time and compares the list. If a file in client side
is missing in server, it is transferred.

Once server has the copies of all the files, server and client runs simple
checksum to ensure server has the "correct" copies. If server side checksum
does not match with that from client, the specific file is overwritten with
a fresh copy from the client

The client side journal is of fixed size and when it nears its capacity,
the earliest written files are deleted to make room

This implementation will mean server side async can be reliably used in
environments where data loss cannot be tolerated.

client side async writes the buffer to server only when memory is full or
during shutdown. This means data loss can occur if client improperly
shutdowns.

the solution is to implement two values max_count and current_count 
accesible in server.

max_count represents the maximum clients that can connect to server at a
time and current_count represents the number of clients connected at present

if current_count < max_count, client can write to the server 

clients will connect to the server at random intervals to check curent_count
and writes if current_count < max_count

There is no hurry as clients have buffers as big as their main memory

The above solution will decrease traffic in the networking allowing for
faster read speeds from the NFS server while reducing data corruption.

created a 2.0 GB file on home directory which is NFS using dd:
dd if=/dev/zero of=/home/user_1/file bs=512M count=4

Copied the file using rsync --progress to /var/tmp which is native FS

resulted in 54.35 MB/s -> Very fast read speeds
